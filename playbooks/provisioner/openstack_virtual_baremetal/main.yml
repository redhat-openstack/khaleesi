---
- name: Pre-provisioning tasks
  hosts: localhost
  sudo: no
  tasks:
    - name: Group by provisioner type
      group_by: key={{ provisioner.type }}

    - name: ensure correct key_file permission
      file: path={{ provisioner.key_file }} mode=0600

    - name: Add the host to the inventory
      add_host:
        name="host0"
        groups="provisioned"
        ansible_fqdn="{{ lookup('env', 'TEST_MACHINE') }}"
        ansible_ssh_user="{{ provisioner.remote_user }}"
        ansible_ssh_private_key_file="{{ provisioner.key_file }}"
        ansible_ssh_host="{{ lookup('env', 'TEST_MACHINE') }}"

- name: set up host cloud environment
  hosts: host0
  tasks:
    - name: set fact stack user home
      set_fact: instack_user_home=/home/{{ provisioner.remote_user }}

    - name: get the guest-image
      get_url: >
        url="{{ provisioner.image.remote_file_server }}{{ provisioner.image.name }}.qcow2"
        dest=/home/stack/{{ provisioner.image.name }}.qcow2

    - name: get admin password
      register: get_admin_password_result
      shell: >
            grep 'OS_PASSWORD' {{ instack_user_home }}/overcloudrc | cut -d '=' -f2

    - name: get auth url
      register: get_auth_url_result
      shell: >
            grep 'OS_AUTH_URL' {{ instack_user_home }}/overcloudrc | cut -d '=' -f2

    - name: create an empty base image for the baremetal instances
      shell: >
        source {{ instack_user_home }}/overcloudrc;
        qemu-img create -f qcow2 empty.qcow2 40G;

    - name: upload empty image
      register: glance_empty_image_uuid_result
      glance_image:
          auth_url: "{{ get_auth_url_result.stdout }}"
          login_username: "admin"
          login_password: "{{ get_admin_password_result.stdout }}"
          login_tenant_name: "admin"
          name: "empty"
          container_format: bare
          disk_format: qcow2
          state: present
          file: "/home/stack/empty.qcow2"

    - name: upload os image
      register: glance_os_image_uuid_result
      glance_image:
          auth_url: "{{ get_auth_url_result.stdout }}"
          login_username: "admin"
          login_password: "{{ get_admin_password_result.stdout }}"
          login_tenant_name: "admin"
          name: "{{ provisioner.image.name }}"
          container_format: bare
          disk_format: qcow2
          state: present
          file: "/home/stack/{{ provisioner.image.name }}.qcow2"

    - name: get admin tenant id
      register: admin_tenant_result
      shell: >
        source {{ instack_user_home }}/overcloudrc;
        keystone tenant-list | grep admin | cut -d ' ' -f2

    - name: set nova quota to unlimited
      ignore_errors: true
      shell: >
        source {{ instack_user_home }}/overcloudrc;
        nova quota-update --ram -1 {{ admin_tenant_result.stdout }};
        nova quota-update --core -1 {{ admin_tenant_result.stdout }};
        nova quota-update --instances -1 {{ admin_tenant_result.stdout }};
        nova quota-update --floating_ips -1 {{ admin_tenant_result.stdout }};

    - name: set neutron subnet quota to unlimited
      ignore_errors: true
      shell: >
        source {{ instack_user_home }}/overcloudrc;
        neutron quota-update --subnet -1;
        neutron quota-update --network -1;
        neutron quota-update --router -1;
        neutron quota-update --security_group -1;

    - name: create baremetal flavor
      shell: >
        source {{ instack_user_home }}/overcloudrc;
        nova flavor-create baremetal auto 6144 50 2;
      ignore_errors: true

    - name: create bmc flavor
      shell: >
        source {{ instack_user_home }}/overcloudrc;
        nova flavor-create bmc auto 512 20 1
      ignore_errors: true

    - name: create provisioning network
      register: provision_network_uuid_result
      quantum_network:
        state: present
        auth_url: "{{ get_auth_url_result.stdout }}"
        login_username: admin
        login_password: "{{ get_admin_password_result.stdout }}"
        login_tenant_name: admin
        name: "{{ tmp.node_prefix }}provision"

    - name: find out how many provision* subnets there are
      shell: >
        source {{ instack_user_home }}/overcloudrc;
        neutron net-list | grep -o 'provision' | wc -l
      register: number_provision_subnets

    - name: create provision subnet
      register: provision_subnet_uuid_result
      ovb_subnet:
        state: present
        auth_url: "{{ get_auth_url_result.stdout }}"
        login_username: admin
        login_password: "{{ get_admin_password_result.stdout }}"
        login_tenant_name: admin
        network_name: "{{ tmp.node_prefix }}provision"
        name: "{{ tmp.node_prefix }}provision"
        enable_dhcp: False
        cidr: "{{ provisioner.host_cloud_networks.provision.cidr }}"

    - name: create private network
      register: private_network_uuid_result
      quantum_network:
        state: present
        auth_url: "{{ get_auth_url_result.stdout }}"
        login_username: admin
        login_password: "{{ get_admin_password_result.stdout }}"
        login_tenant_name: admin
        name: "{{ tmp.node_prefix }}private"

    - name: find out how many private* subnets there are
      shell: >
        source {{ instack_user_home }}/overcloudrc;
        neutron net-list | grep -o 'private' | wc -l
      register: number_private_subnets

    - name: create private subnet
      register: private_subnet_uuid_result
      ovb_subnet:
        state: present
        auth_url: "{{ get_auth_url_result.stdout }}"
        login_username: admin
        login_password: "{{ get_admin_password_result.stdout }}"
        login_tenant_name: admin
        network_name: "{{ tmp.node_prefix }}private"
        name: "{{ tmp.node_prefix }}private"
        dns_nameservers: "{{ hw_env.dns_server }}"
        cidr: "{{ provisioner.host_cloud_networks.private.cidr }}"

    - name: create router
      quantum_router:
        state: present
        auth_url: "{{ get_auth_url_result.stdout }}"
        login_username: admin
        login_password: "{{ get_admin_password_result.stdout }}"
        login_tenant_name: admin
        name: "{{ tmp.node_prefix }}router"

    - name: create router gateway - nova
      quantum_router_gateway:
        state: present
        auth_url: "{{ get_auth_url_result.stdout }}"
        login_username: admin
        login_password: "{{ get_admin_password_result.stdout }}"
        login_tenant_name: admin
        router_name: "{{ tmp.node_prefix }}router"
        network_name: nova

    - name: create router interface - private
      quantum_router_interface:
        state: present
        auth_url: "{{ get_auth_url_result.stdout }}"
        login_username: admin
        login_password: "{{ get_admin_password_result.stdout }}"
        login_tenant_name: admin
        router_name: "{{ tmp.node_prefix }}router"
        subnet_name: "{{ tmp.node_prefix }}private"

    - name: create a public network
      register: public_network_uuid_result
      quantum_network:
        state: present
        auth_url: "{{ get_auth_url_result.stdout }}"
        login_username: admin
        login_password: "{{ get_admin_password_result.stdout }}"
        login_tenant_name: admin
        name: "{{ tmp.node_prefix }}public"

    - name: create public subnet
      register: public_subnet_uuid_result
      ovb_subnet:
        state: present
        auth_url: "{{ get_auth_url_result.stdout }}"
        login_username: admin
        login_password: "{{ get_admin_password_result.stdout }}"
        login_tenant_name: admin
        network_name: "{{ tmp.node_prefix }}public"
        name: "{{ tmp.node_prefix }}public"
        dns_nameservers: "{{ hw_env.dns_server }}"
        cidr: "{{ provisioner.host_cloud_networks.public.cidr }}"
        enable_dhcp: False

    - name: create default nova keypair
      shell: >
        source {{ instack_user_home }}/overcloudrc;
        nova keypair-add --pub_key ~/.ssh/id_rsa.pub "{{ tmp.node_prefix }}default"

    - name: copy the example env file and edit it to reflect the host environment
      shell: >
        source {{ instack_user_home }}/overcloudrc;
        cp {{ instack_user_home }}/openstack-virtual-baremetal/templates/env.yaml.example {{ instack_user_home }}/openstack-virtual-baremetal/env.yaml;
        sed -i "s#http://10.0.0.1:5000/v2.0#$OS_AUTH_URL#g" {{ instack_user_home }}/openstack-virtual-baremetal/env.yaml;
        sed -i 's/os_password: password/os_password: '"$OS_PASSWORD"'/g'  {{ instack_user_home }}/openstack-virtual-baremetal/env.yaml;
        sed -i 's/CentOS-7-x86_64-GenericCloud-1503/{{ provisioner.image.name }}/g' {{ instack_user_home }}/openstack-virtual-baremetal/env.yaml;
        sed -i 's/node_count: 2/node_count: {{ provisioner.node_count }}/g'  {{ instack_user_home }}/openstack-virtual-baremetal/env.yaml;
        sed -i 's/private_net: private/private_net: {{ tmp.node_prefix }}private/g' {{ instack_user_home }}/openstack-virtual-baremetal/env.yaml;
        sed -i 's/public_net: public/public_net: {{ tmp.node_prefix }}public/g' {{ instack_user_home }}/openstack-virtual-baremetal/env.yaml;
        sed -i 's/provision_net: provision/provision_net: {{ tmp.node_prefix }}provision/g' {{ instack_user_home }}/openstack-virtual-baremetal/env.yaml;
        sed -i 's/bmc_prefix: bmc/bmc_prefix: {{ tmp.node_prefix }}bmc/g' {{ instack_user_home }}/openstack-virtual-baremetal/env.yaml;
        sed -i 's/baremetal_prefix: baremetal/baremetal_prefix: {{ tmp.node_prefix }}baremetal/g' {{ instack_user_home }}/openstack-virtual-baremetal/env.yaml;
        sed -i 's/key_name: default/key_name: {{ tmp.node_prefix }}default/g' {{ instack_user_home }}/openstack-virtual-baremetal/env.yaml;
        sed -i 's/bmc_flavor: bmc/bmc_flavor: m1.small/g' {{ instack_user_home }}/openstack-virtual-baremetal/env.yaml;
        sed -i 's/external_net: external/external_net: nova/g' {{ instack_user_home }}/openstack-virtual-baremetal/env.yaml;

    - name: change from centos-related to RHEL-related repos
      shell: >
         chdir={{ instack_user_home }}/openstack-virtual-baremetal
         sed -i 's#yum -y update centos-release#yum localinstall -y {{ product.rpm }}#g' templates/virtual-baremetal-servers.yaml
      when: provisioner.distro.name == "rhel"

    - name: Execute rhos-release for pinned osp-director puddle (osp)
      lineinfile: >
        dest={{ instack_user_home }}/openstack-virtual-baremetal/templates/virtual-baremetal-servers.yaml 
        line="            rhos-release -p {{ product.repo.puddle_director_pin_version }} {{ product.full_version }}"
        state=present
        insertafter="^            yum localinstall"
      when: product.repo_type in ['puddle'] and product.repo.puddle_director_pin_version is defined

    - name: Execute rhos-release for core rhos puddle (osp)
      shell: > 
         chdir={{ instack_user_home }}/openstack-virtual-baremetal
         sed -i 's#yum install -y https://rdo.fedorapeople.org/rdo-release.rpm#rhos-release -p {{ product.repo.puddle_pin_version }} {{ product.repo.core_product_version }}#g' templates/virtual-baremetal-servers.yaml
      when: product.repo_type in ['puddle'] and product.repo.puddle_pin_version is defined

    - name: Execute rhos-release for pinned osp-director poodle (osp)
      shell: >
         chdir={{ instack_user_home }}/openstack-virtual-baremetal
         sed -i 's#yum install -y https://rdo.fedorapeople.org/rdo-release.rpm#rhos-release  -d -p {{ product.repo.poodle_pin_version }} {{ product.full_version }}#g' templates/virtual-baremetal-servers.yaml
      when: product.repo_type in ['poodle'] and product.repo.poodle_pin_version is defined

    - name: Execute rhos-release for osp-director puddle (osp)
      shell: >
         chdir={{ instack_user_home }}/openstack-virtual-baremetal
         sed -i 's#yum install -y https://rdo.fedorapeople.org/rdo-release.rpm#rhos-release {{ product.full_version }}}#g' templates/virtual-baremetal-servers.yaml
      when: product.repo_type in ['puddle'] and product.repo.puddle_pin_version is not defined

    - name: Execute rhos-release for osp-director poodle (osp)
      shell: >
         chdir={{ instack_user_home }}/openstack-virtual-baremetal
         sed -i 's#yum install -y https://rdo.fedorapeople.org/rdo-release.rpm#rhos-release  -d {{ product.full_version }}#g' templates/virtual-baremetal-servers.yaml
      when: product.repo_type in ['poodle'] and product.repo.poodle_pin_version is not defined

    - name: deploy the stack
      shell: >
        chdir={{ instack_user_home }}/openstack-virtual-baremetal
        source {{ instack_user_home }}/overcloudrc;
        op=${1:-create};
        heat stack-$op -f templates/virtual-baremetal.yaml -e env.yaml -e templates/resource-registry.yaml "{{product.name}}-{{ tmp.node_prefix }}baremetal"

    - name: wait for heat stack to go to complete
      shell: >
        source {{ instack_user_home }}/overcloudrc;
        heat stack-list | grep "{{product.name}}-{{ tmp.node_prefix }}baremetal"
      register: heat_stack_list_result
      until: heat_stack_list_result.stdout.find("COMPLETE") != -1
      retries: 10
      delay: 180

    - name: view baremetal nodes
      shell: >
        source {{ instack_user_home }}/overcloudrc;
        heat stack-show "{{product.name}}-{{ tmp.node_prefix }}baremetal"

    - name: get nova network uuid
      shell: >
        source {{ instack_user_home }}/overcloudrc;
        neutron net-list | grep 'nova' | sed -e 's/|//g' | awk '{print $1}'
      register: nova_network_uuid

    - name: add security group tcp rule
      shell: >
        source {{ instack_user_home }}/overcloudrc;
        nova secgroup-add-rule default tcp 22 22 0.0.0.0/0
      ignore_errors: true

    - name: add security group icmp rule
      shell: >
        source {{ instack_user_home }}/overcloudrc;
        nova secgroup-add-rule default icmp -1 -1 0.0.0.0/0
      ignore_errors: true

    - name: boot an instance to serve as the undercloud
      shell: >
        source {{ instack_user_home }}/overcloudrc;
        nova boot "{{ tmp.node_prefix }}undercloud" --flavor m1.xlarge --image {{ provisioner.image.name }} --nic net-id={{ private_network_uuid_result.id }} --nic net-id={{ provision_network_uuid_result.id }} --nic net-id={{ public_network_uuid_result.id }} --key_name "{{ tmp.node_prefix }}default" --security_group default

    - name: wait for undercloud instance to go active
      shell: >
        source {{ instack_user_home }}/overcloudrc;
        nova list | grep "{{ tmp.node_prefix }}undercloud"
      register: nova_list_result
      until: nova_list_result.stdout.find("ACTIVE") != -1
      retries: 5
      delay: 60

    - name: check what floating_ips exist
      shell: >
        source {{ instack_user_home }}/overcloudrc;
        neutron floatingip-list
      register: floatingip_list

    - name: get public port uuid
      register: public_port_result
      shell: >
            source {{ instack_user_home }}/overcloudrc;
            neutron port-list  | grep {{ public_subnet_uuid_result.id }} | cut -d " " -f2

    - name: Delete public port to baremetal instance
      shell: >
            source {{ instack_user_home }}/overcloudrc;
            neutron port-delete {{ item }}
      with_items: public_port_result.stdout_lines

    - name: create floating ip
      shell: >
        source {{ instack_user_home }}/overcloudrc;
        neutron floatingip-create {{ nova_network_uuid.stdout }}
      register: floating_ip_created

    - name: get floating ip id
      shell: >
        source {{ instack_user_home }}/overcloudrc;
        echo "{{ floating_ip_created.stdout_lines.7 }}" | grep ' id ' | sed -e 's/|//g' | awk '{print $2}'
      register: floating_ip_created_id

    - name: get floating ip address
      shell: >
        source {{ instack_user_home }}/overcloudrc;
        neutron floatingip-show '{{ floating_ip_created_id.stdout }}' | grep 'floating_ip_address' | sed -e 's/|//g' | awk '{print $2}'
      register: floating_ip_result

    - name: set fact for floating_ip
      set_fact: floating_ip={{ floating_ip_result.stdout }}

    - name: get the private ip on the undercloud instance
      shell: >
        source {{ instack_user_home }}/overcloudrc;
        nova show "{{ tmp.node_prefix }}undercloud" | grep '{{ tmp.node_prefix }}private network' | sed -e 's/|//g' | awk '{print $3}'
      register: undercloud_private_ip

    - name: get undercloud instance port id
      shell: >
        source {{ instack_user_home }}/overcloudrc;
        neutron port-list | grep '{{ undercloud_private_ip.stdout }}' | sed -e 's/|//g' | awk '{print $1}'
      register: undercloud_port_id

    - name: associate floating ip with undercloud instance
      shell: >
        source {{ instack_user_home }}/overcloudrc;
        neutron floatingip-associate {{ floating_ip_created_id.stdout }} {{ undercloud_port_id.stdout }}

    - name: build a nodes.json file that can be imported into Ironic
      shell: >
        chdir={{ instack_user_home }}/openstack-virtual-baremetal
        source {{ instack_user_home }}/overcloudrc;
        bin/build-nodes-json --bmc_prefix="{{ tmp.node_prefix }}bmc" --baremetal_prefix="{{ tmp.node_prefix }}baremetal" --private_net="{{ tmp.node_prefix }}private" --provision_net="{{ tmp.node_prefix }}provision"

    - name: wait for nodes.json to exist
      wait_for: path="{{ instack_user_home }}/openstack-virtual-baremetal/nodes.json"

    - name: move nodes.json file to run-specific file
      shell: "mv {{ instack_user_home }}/openstack-virtual-baremetal/nodes.json {{ instack_user_home }}/openstack-virtual-baremetal/{{ tmp.node_prefix }}nodes.json"

    - name: wait for run-specific nodes.json to exist
      wait_for: path="{{ instack_user_home }}/openstack-virtual-baremetal/{{ tmp.node_prefix }}nodes.json"

    - name: get number of nodes available
      shell: >
        chdir={{ instack_user_home }}/openstack-virtual-baremetal
        grep -o 'pm_password' "{{ tmp.node_prefix }}nodes.json" | wc -l
      register: node_number

    - name: Wait for ssh to be available on undercloud instance
      register: wait_for_ssh
      wait_for: host={{ floating_ip_result.stdout }} port=22 delay=60 timeout=600

    - name: copy nodes.json as instackenv.json on undercloud instance
      shell: >
        source {{ instack_user_home }}/overcloudrc;
        scp -o "StrictHostKeyChecking=no" {{ instack_user_home }}/openstack-virtual-baremetal/{{ tmp.node_prefix }}nodes.json {{ provisioner.host_cloud_user }}@{{ floating_ip_result.stdout }}:~/instackenv.json

    - name: copy instackenv.json to root dir
      shell: 'ssh -t -o "StrictHostKeyChecking=no" {{ provisioner.host_cloud_user }}@{{ floating_ip_result.stdout }} "sudo cp /home/{{ provisioner.host_cloud_user }}/instackenv.json /root/instackenv.json"'
      when: provisioner.host_cloud_user != 'root'

    - name: copy authorized_keys to root user
      shell: 'ssh -t -o "StrictHostKeyChecking=no" {{ provisioner.host_cloud_user }}@{{ floating_ip_result.stdout }} "sudo rm -rf /root/.ssh/authorized_keys; sudo cp /home/{{ provisioner.host_cloud_user }}/.ssh/authorized_keys /root/.ssh/"'
      ignore_errors: true

    - name: copy ssh keys to undercloud instance user
      shell: >
        source {{ instack_user_home }}/overcloudrc;
        scp -o "StrictHostKeyChecking=no" .ssh/id_rsa {{ provisioner.host_cloud_user }}@{{ floating_ip_result.stdout }}:~/.ssh/id_rsa

    - name: copy ssh pub keys to undercloud instance user
      shell: >
        source {{ instack_user_home }}/overcloudrc;
        scp -o "StrictHostKeyChecking=no" .ssh/id_rsa.pub {{ provisioner.host_cloud_user }}@{{ floating_ip_result.stdout }}:~/.ssh/id_rsa.pub

    - name: copy ssh keys to root dir
      shell: 'ssh -t -o "StrictHostKeyChecking=no" {{ provisioner.host_cloud_user }}@{{ floating_ip_result.stdout }} "sudo cp /home/{{ provisioner.host_cloud_user }}/.ssh/id_rsa /root/.ssh/id_rsa"'
      when: provisioner.host_cloud_user != 'root'

    - name: copy ssh pub keys to root dir
      shell: 'ssh -t -o "StrictHostKeyChecking=no" {{ provisioner.host_cloud_user }}@{{ floating_ip_result.stdout }} "sudo cp /home/{{ provisioner.host_cloud_user }}/.ssh/id_rsa.pub /root/.ssh/id_rsa.pub"'
      when: provisioner.host_cloud_user != 'root'

    - name: setup ssh config
      template: src={{ base_dir }}/khaleesi/playbooks/installer/rdo-manager/templates/ssh_config.j2 dest=~/ssh.config.ansible mode=0755

    - name: copy ssh_config back to the slave
      fetch: src=~/ssh.config.ansible dest={{ base_dir }}/khaleesi/ssh.config.ansible flat=yes

    - name: change mod for ssh.config.ansible
      local_action: shell chmod 755 {{ base_dir }}/khaleesi/ssh.config.ansible

    - name: copy id_rsa key back to the slave
      fetch: src=~/.ssh/id_rsa dest={{ base_dir }}/khaleesi/id_rsa_undercloud_instance flat=yes

- name: set fact for the overcloud deployment
  hosts: localhost
  tasks:
    - name: get floating ip
      shell: "cat {{ base_dir }}/khaleesi/ssh.config.ansible | grep 'Hostname' | sed -n -e 's/    Hostname //p'"
      register: floating_ip_result

    - name: set fact for floating_ip
      set_fact: floating_ip={{ floating_ip_result.stdout }}
